{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3주차코드",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CbwtB34-yAq"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
        "from tqdm import tqdm\n",
        "import lightgbm as lgbm\n",
        "import re\n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "\n",
        "import os\n",
        "import json\n",
        "%matplotlib inline\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.layers import Dense, Activation, Conv2D,Conv1D, Flatten,MaxPooling2D,BatchNormalization,Lambda, AveragePooling2D, MaxPooling1D\n",
        "import keras.backend as K\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, Callback, EarlyStopping\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNAzX-aGAGf5",
        "outputId": "2583bfe4-c338-4209-bc76-592f8fa23a30"
      },
      "source": [
        "print(\"keras_version : \" + str(keras.__version__))\n",
        "print(\"numpy_version : \" + str(np.__version__))\n",
        "print(\"pandas_version : \" + str(pd.__version__))\n",
        "print(\"tensorflow_version :\" + str(tf.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keras_version : 2.4.3\n",
            "numpy_version : 1.18.5\n",
            "pandas_version : 1.1.4\n",
            "tensorflow_version :2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWsjNPLWAkBY"
      },
      "source": [
        "## 2. 데이터 전처리\n",
        "## Data Cleansing & Pre-Processing  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj-IwWKyAoFl"
      },
      "source": [
        "## 1. Case1, Case2 (변수추가 + Time 200까지만) \n",
        "Case1 : Cosine scheduler 사용 \\\n",
        "Case2 : X \\\n",
        "-> 데이터가 적고, metric이 너무 예민해서 그런지, scheduler 혹은 lr에  따라 학습 되는 것이 너무 달라 앙상블 해줬습니다.\\\n",
        "-> LB : 0.0035"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiFTSdU4AHVs",
        "outputId": "5e8affe7-3786-4baa-e22b-d3b94cda8e16"
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/train_features.csv\", index_col=0)\n",
        "test = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/test_features.csv\", index_col=0)\n",
        "y_train = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/train_target.csv\", index_col=0)\n",
        "y_test = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/sample_submission.csv\", index_col=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJxvkBvhAUlL"
      },
      "source": [
        "train = train.loc[train['Time'] <= 0.000004*200]\n",
        "test = test.loc[test['Time'] <= 0.000004*200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK9RVqeeAaH6"
      },
      "source": [
        "def transform1(train, n1, n2):\n",
        "    train_time = train.copy()\n",
        "    train_time['S1'] = [1 if i != 0 else 0 for i in train['S1']]\n",
        "    train_time['S2'] = [1 if i != 0 else 0 for i in train['S2']]\n",
        "    train_time['S3'] = [1 if i != 0 else 0 for i in train['S3']]\n",
        "    train_time['S4'] = [1 if i != 0 else 0 for i in train['S4']]\n",
        "    train_time.drop(['Time'], axis=1, inplace=True)\n",
        "    \n",
        "    train_time = train_time.sum(axis=1)\n",
        "    train_time = train_time.transform(lambda x: (x-x.mean())/(x.std()))\n",
        "    train_time = np.array(train_time).reshape(n1, n2, 1, 1)\n",
        "    \n",
        "    return train_time\n",
        "\n",
        "train_time = transform1(train, 2800, 200)\n",
        "test_time = transform1(test, 700, 200)\n",
        "\n",
        "train = train.transform(lambda x: (x-x.mean())/(x.std()))\n",
        "test = test.transform(lambda x: (x-x.mean())/(x.std()))\n",
        "\n",
        "###\n",
        "\n",
        "X_data = train.iloc[:,:]\n",
        "X_data = np.array(X_data).reshape((2800,200,5,1))\n",
        "\n",
        "X_data_test = test.iloc[:,:]\n",
        "X_data_test = np.array(X_data_test).reshape((700,200,5,1))\n",
        "\n",
        "Y_data = np.array(y_train.copy())\n",
        "\n",
        "### Merge\n",
        "\n",
        "X_data = np.concatenate([X_data, train_time], axis=2)\n",
        "X_data_test = np.concatenate([X_data_test, test_time], axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ4D6sNXAc22"
      },
      "source": [
        "X_data_Case12 = X_data.copy()\n",
        "X_data_test_Case12 = X_data_test.copy()\n",
        "Y_data_Case12 = Y_data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIEuv_AjAehd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv3xtKhfAz2v"
      },
      "source": [
        "## 3.  Case3 (kernel regularizer 사용)\n",
        "앙상블을 하면 어떨까 해서 만들어 봤습니다. \\\n",
        "변수만추가하고, kerenl regularizer을 사용하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpoPxQzhA0XV",
        "outputId": "e44dfb51-534d-4810-8013-e706e96a8ebf"
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/train_features.csv\", index_col=0)\n",
        "test = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/test_features.csv\", index_col=0)\n",
        "y_train = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/train_target.csv\", index_col=0)\n",
        "y_test = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/sample_submission.csv\", index_col=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2NKYF0ZA27T"
      },
      "source": [
        "train_time = transform1(train, 2800, 375)\n",
        "test_time = transform1(test, 700, 375)\n",
        "\n",
        "train = train.transform(lambda x: (x-x.mean())/(x.std()))\n",
        "test = test.transform(lambda x: (x-x.mean())/(x.std()))\n",
        "\n",
        "###\n",
        "\n",
        "X_data = train.iloc[:,:]\n",
        "X_data = np.array(X_data).reshape((2800,375,5,1))\n",
        "\n",
        "X_data_test = test.iloc[:,:]\n",
        "X_data_test = np.array(X_data_test).reshape((700,375,5,1))\n",
        "\n",
        "Y_data = np.array(y_train.copy())\n",
        "\n",
        "### Merge\n",
        "\n",
        "X_data = np.concatenate([X_data, train_time], axis=2)\n",
        "X_data_test = np.concatenate([X_data_test, test_time], axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-_Bh9iHA1tG"
      },
      "source": [
        "X_data_Case3 = X_data.copy()\n",
        "X_data_test_Case3 = X_data_test.copy()\n",
        "Y_data_Case3 = Y_data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59rJR-G4A8hH"
      },
      "source": [
        "## 4.  Case4\n",
        "baseline model입니다. 유용균박사님 코드에서 cosine scheduler만 추가한 것입니다.\\\n",
        "이 또한 앙상블을 하기 위해 만들어 봤습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWooLr4uA9p0",
        "outputId": "d20a70bb-50cc-41a9-bd0f-059c882de73c"
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/train_features.csv\", index_col=0)\n",
        "test = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/test_features.csv\", index_col=0)\n",
        "y_train = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/train_target.csv\", index_col=0)\n",
        "y_test = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/sample_submission.csv\", index_col=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57Y2-UFiBAQr"
      },
      "source": [
        "train = train.transform(lambda x: (x-x.mean())/(x.std()))\n",
        "test = test.transform(lambda x: (x-x.mean())/(x.std()))\n",
        "\n",
        "###\n",
        "\n",
        "X_data = train.iloc[:,:]\n",
        "X_data = np.array(X_data).reshape((2800,375,5,1))\n",
        "\n",
        "X_data_test = test.iloc[:,:]\n",
        "X_data_test = np.array(X_data_test).reshape((700,375,5,1))\n",
        "\n",
        "Y_data = np.array(y_train.copy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B8kD7XnBAf-"
      },
      "source": [
        "X_data_Case4 = X_data.copy()\n",
        "X_data_test_Case4 = X_data_test.copy()\n",
        "Y_data_Case4 = Y_data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSX8d4qqBBo0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AW535YgBE1z"
      },
      "source": [
        "## check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY8XbVEbBFHw",
        "outputId": "358daada-f5ec-4e01-ea28-173ce643811a"
      },
      "source": [
        "print(\"Case12 nrow: \" + str(X_data_Case12.shape[1]) + \" / Case12 ncol : \" + str(X_data_Case12.shape[2]))\n",
        "print(\"Case3 nrow : \" + str(X_data_Case3.shape[1]) + \" / Case3 ncol : \" + str(X_data_Case3.shape[2]))\n",
        "print(\"Case4 nrow : \" + str(X_data_Case4.shape[1]) + \" / Case4 ncol : \" + str(X_data_Case4.shape[2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Case12 nrow: 200 / Case12 ncol : 6\n",
            "Case3 nrow : 375 / Case3 ncol : 6\n",
            "Case4 nrow : 375 / Case4 ncol : 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioSPaiWrBGcx"
      },
      "source": [
        "import math\n",
        "from keras.callbacks import Callback\n",
        "class CosineAnnealingScheduler(Callback):\n",
        "    \"\"\"Cosine annealing scheduler.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
        "        super(CosineAnnealingScheduler, self).__init__()\n",
        "        self.T_max = T_max\n",
        "        self.eta_max = eta_max\n",
        "        self.eta_min = eta_min\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if not hasattr(self.model.optimizer, 'lr'):\n",
        "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "        #if self.verbose &gt; 0:\n",
        "        #    print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
        "        #          'rate to %s.' % (epoch + 1, lr))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9p0VtDYBJON"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def kaeri_metric(y_true, y_pred):\n",
        "    '''\n",
        "    y_true: dataframe with true values of X,Y,M,V\n",
        "    y_pred: dataframe with pred values of X,Y,M,V\n",
        "    \n",
        "    return: KAERI metric\n",
        "    '''\n",
        "    \n",
        "    return 0.5 * E1(y_true, y_pred) + 0.5 * E2(y_true, y_pred)\n",
        "\n",
        "\n",
        "### E1과 E2는 아래에 정의됨 ###\n",
        "\n",
        "def E1(y_true, y_pred):\n",
        "    '''\n",
        "    y_true: dataframe with true values of X,Y,M,V\n",
        "    y_pred: dataframe with pred values of X,Y,M,V\n",
        "    \n",
        "    return: distance error normalized with 2e+04\n",
        "    '''\n",
        "    \n",
        "    _t, _p = np.array(y_true)[:,:2], np.array(y_pred)[:,:2]\n",
        "    \n",
        "    return np.mean(np.sum(np.square(_t - _p), axis = 1) / 2e+04)\n",
        "\n",
        "\n",
        "def E2(y_true, y_pred):\n",
        "    '''\n",
        "    y_true: dataframe with true values of X,Y,M,V\n",
        "    y_pred: dataframe with pred values of X,Y,M,V\n",
        "    \n",
        "    return: sum of mass and velocity's mean squared percentage error\n",
        "    '''\n",
        "    \n",
        "    _t, _p = np.array(y_true)[:,2:], np.array(y_pred)[:,2:]\n",
        "    \n",
        "    \n",
        "    return np.mean(np.sum(np.square((_t - _p) / (_t + 1e-06)), axis = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-bmQLLdBLA8"
      },
      "source": [
        "weight1 = np.array([1,1,0,0])\n",
        "weight2 = np.array([0,0,1,1])\n",
        "\n",
        "def my_loss(y_true, y_pred):\n",
        "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
        "    return K.mean(K.square(divResult))\n",
        "\n",
        "\n",
        "def my_loss_E1(y_true, y_pred):\n",
        "    return K.mean(K.square(y_true-y_pred)*weight1)/2e+04\n",
        "\n",
        "def my_loss_E2(y_true, y_pred):\n",
        "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
        "    return K.mean(K.square(divResult)*weight2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycPbuRx0BMr5"
      },
      "source": [
        "def plot_error(type_id,pred,true):\n",
        "    print(pred.shape)\n",
        "\n",
        "    if type_id == 0:\n",
        "        _name = 'x_pos'\n",
        "    elif type_id == 1:\n",
        "        _name = 'y_pos'\n",
        "    elif type_id == 2:\n",
        "        _name = 'mass'\n",
        "    elif type_id == 3:\n",
        "        _name = 'velocity'\n",
        "    elif type_id == 4:\n",
        "        _name = \"distance\"\n",
        "    else:\n",
        "        _name = 'error'\n",
        "\n",
        "    x_coord = np.arange(1,pred.shape[0]+1,1)\n",
        "    if type_id < 2:\n",
        "        Err_m = (pred[:,type_id] - true[:,type_id])\n",
        "    elif type_id < 4:\n",
        "        Err_m = ((pred[:,type_id] - true[:,type_id])/true[:,type_id])*100\n",
        "    else:\n",
        "        Err_m = ((pred[:,0]-true[:,0])**2+(pred[:,1]-true[:,1])**2)**0.5\n",
        "\n",
        "\n",
        "    fig = plt.figure(figsize=(8,6))\n",
        "    # plt.rcParams[\"font.family\"]=\"Times New Roman\"\n",
        "    plt.rcParams[\"font.size\"]=15\n",
        "    plt.scatter(x_coord, Err_m, marker='o')\n",
        "    plt.title(\"%s Prediction for Training Data\" % _name, size=20)\n",
        "    plt.xlabel(\"Data ID\", labelpad=10, size=20)\n",
        "    plt.ylabel(\"Prediction Error of %s,\" % _name, labelpad=10, size=20)\n",
        "    plt.xticks(size=15)\n",
        "    plt.yticks(size=15)\n",
        "    plt.ylim(-100., 100.)\n",
        "    plt.xlim(0, pred.shape[0]+1)\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    print(np.std(Err_m))\n",
        "    print(np.max(Err_m))\n",
        "    print(np.min(Err_m))\n",
        "    return Err_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzG9H5xoBOUw"
      },
      "source": [
        "def set_model(train_target, n_sam,n_col, regularizer_True):  # 0:x,y, 1:m, 2:v\n",
        "    \n",
        "    activation = 'elu'\n",
        "    padding = 'same'\n",
        "    model = Sequential()\n",
        "    nf = 16\n",
        "    fs = (3,1)\n",
        "    \n",
        "    model.add(Conv2D(nf,fs, padding=padding, activation=activation,input_shape=(n_sam,n_col,1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*2,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*4,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*8,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*16,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*32,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation ='elu'))\n",
        "    model.add(Dense(64, activation ='elu'))\n",
        "    model.add(Dense(32, activation ='elu'))\n",
        "    model.add(Dense(16, activation ='elu'))\n",
        "    \n",
        "    if regularizer_True:\n",
        "        model.add(Dense(4, kernel_regularizer=l1_l2(l1=0.001)))\n",
        "    else:\n",
        "        model.add(Dense(4))\n",
        "    \n",
        "\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "    #optimizer = keras.optimizers.RMSprop()\n",
        "    \n",
        "    global weight2\n",
        "    if train_target == 1: # only for M\n",
        "        weight2 = np.array([0,0,1,0])\n",
        "    else: # only for V\n",
        "        weight2 = np.array([0,0,0,1])\n",
        "       \n",
        "    if train_target==0:\n",
        "        model.compile(loss=my_loss_E1,\n",
        "                  optimizer=optimizer,\n",
        "                 )\n",
        "    else:\n",
        "        model.compile(loss=my_loss_E2,\n",
        "                  optimizer=optimizer,\n",
        "                 )\n",
        "       \n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKoA04RWBQU-"
      },
      "source": [
        "def train_CNN(model,X,Y,X_val,Y_val, save_point, enum, train_target, scheduler_True):\n",
        "    \n",
        "    from numpy.random import seed\n",
        "    seed(777)\n",
        "    from tensorflow import set_random_seed\n",
        "    set_random_seed(777)\n",
        "    \n",
        "    MODEL_SAVE_FOLDER_PATH = './model/'\n",
        "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
        "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
        "\n",
        "    model_path = MODEL_SAVE_FOLDER_PATH + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "    best_save = ModelCheckpoint('best_m_' + str(save_point) + \"_\" + str(enum) + \"_\" + str(train_target) + '.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "    cosine_scheduler = CosineAnnealingScheduler(T_max=100, eta_max=6e-3, eta_min=1e-6)\n",
        "\n",
        "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
        "    \n",
        "    my_seed = 777\n",
        "    np.random.seed(my_seed)\n",
        "    if scheduler_True:\n",
        "        history = model.fit(X, Y,\n",
        "                      epochs=10000,\n",
        "                      batch_size=128,\n",
        "                      shuffle=True,\n",
        "                      validation_data=(X_val, Y_val),\n",
        "                      verbose = 2,\n",
        "                      callbacks=[best_save, early_stop,cosine_scheduler],\n",
        "                      )\n",
        "    else:\n",
        "        history = model.fit(X, Y,\n",
        "                      epochs=10000,\n",
        "                      batch_size=128,\n",
        "                      shuffle=True,\n",
        "                      validation_data=(X_val, Y_val),\n",
        "                      verbose = 2,\n",
        "                      callbacks=[best_save, early_stop],\n",
        "                      )\n",
        "    \n",
        "\n",
        "    fig, loss_ax = plt.subplots()\n",
        "    acc_ax = loss_ax.twinx()\n",
        "\n",
        "    loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
        "    loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "    loss_ax.set_xlabel('epoch')\n",
        "    loss_ax.set_ylabel('loss')\n",
        "    loss_ax.legend(loc='upper left')\n",
        "    plt.show()    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cotpsarQBSTL"
      },
      "source": [
        "def load_best_model(train_target, save_point, enum):\n",
        "\n",
        "    if train_target == 0:\n",
        "        model = load_model('best_m_' + str(save_point) + \"_\" + str(enum) + \"_\" + str(train_target) + '.hdf5' , custom_objects={'my_loss_E1': my_loss, })\n",
        "    else:\n",
        "        model = load_model('best_m_' + str(save_point) + \"_\" + str(enum) + \"_\" + str(train_target) + '.hdf5' , custom_objects={'my_loss_E2': my_loss, })\n",
        "\n",
        "    score = model.evaluate(X_data, Y_data, verbose=0)\n",
        "    print('loss:', score)\n",
        "\n",
        "    pred = model.predict(X_data)\n",
        "\n",
        "    i=0\n",
        "\n",
        "    print('정답(original):', Y_data[i])\n",
        "    print('예측값(original):', pred[i])\n",
        "\n",
        "    print(E1(pred, Y_data))\n",
        "    print(E2(pred, Y_data))\n",
        "    #print(E2M(pred, Y_data))\n",
        "    #print(E2V(pred, Y_data))    \n",
        "    \n",
        "    if train_target ==0:\n",
        "        plot_error(4,pred,Y_data)\n",
        "    elif train_target ==1:\n",
        "        plot_error(2,pred,Y_data)\n",
        "    elif train_target ==2:\n",
        "        plot_error(3,pred,Y_data)    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHmLzARSBUnu"
      },
      "source": [
        "lr = 3e-4\n",
        "lr_d = 0.0\n",
        "patience = 30\n",
        "dr_rate = 0\n",
        "\n",
        "def run_model(X_data, X_data_test, Y_data, save_point,scheduler_True,regularizer_True):\n",
        "    submit = pd.read_csv('sample_submission.csv', index_col=0)\n",
        "    submit.iloc[:,:] = 0\n",
        "\n",
        "    lr = 3e-4\n",
        "    lr_d = 0.0\n",
        "    patience = 30\n",
        "    dr_rate = 0\n",
        "    \n",
        "    n_col = X_data.shape[2]\n",
        "    n_sam = X_data.shape[1]\n",
        "    \n",
        "    for enum, (train_index,valid_index) in enumerate(group_kfold.split(X_data,Y_data,groups)):\n",
        "        X_train = X_data[train_index]\n",
        "        Y_train = Y_data[train_index]\n",
        "    \n",
        "        X_val = X_data[valid_index]\n",
        "        Y_val = Y_data[valid_index]\n",
        "    \n",
        "    \n",
        "        for train_target in range(3):\n",
        "            model = set_model(train_target, n_sam,n_col, regularizer_True)\n",
        "        \n",
        "            train_CNN(model,X_train, Y_train, X_val, Y_val, save_point, enum, train_target,scheduler_True)    \n",
        "            best_model = load_best_model(train_target, save_point, enum)\n",
        "\n",
        "   \n",
        "            pred_data_test = best_model.predict(X_data_test)\n",
        "            pred_data_valid = best_model.predict(X_val)\n",
        "    \n",
        "            if train_target == 0: # x,y 학습\n",
        "                submit.iloc[:,0] += pred_data_test[:,0]/5\n",
        "                submit.iloc[:,1] += pred_data_test[:,1]/5\n",
        "                \n",
        "            elif train_target == 1: # m 학습\n",
        "                submit.iloc[:,2] += pred_data_test[:,2]/5\n",
        "        \n",
        "            elif train_target == 2: # v 학습\n",
        "                submit.iloc[:,3] += pred_data_test[:,3]/5    \n",
        "    \n",
        "    return submit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGEr_0X4BX1Q"
      },
      "source": [
        "## GroupKFold\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzSdg3oHBVBk",
        "outputId": "d66d6edd-7256-41ea-b2cf-9fb3fd26abb3"
      },
      "source": [
        "y_train = pd.read_csv(\"/content/drive/My Drive/Dacon_튜터/train_target.csv\", index_col=0)\n",
        "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.model_selection import GroupKFold\n",
        "group_kfold = GroupKFold(n_splits=5)\n",
        "groups = y_train['X'].astype('str') + \"_\" + y_train['Y'].astype('str') + \\\n",
        "    \"_\" + y_train['M'].astype('str') + \"_\" + y_train['V'].astype('str')\n",
        "group_kfold.get_n_splits(X_data, Y_data, groups)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqzbw50IBd-p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJQksb5SBZTj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6V1V4aFBaoc"
      },
      "source": [
        "## 5. 모델 학습 및 검증\n",
        "## Model Tuning & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RrlGyHwBa7M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COprkycIBcmG"
      },
      "source": [
        "# Case1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hrEskLjBc01"
      },
      "source": [
        "X_data = X_data_Case12.copy()\n",
        "X_data_test = X_data_test_Case12.copy()\n",
        "Y_data = Y_data_Case12.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbDyu0aZBicC"
      },
      "source": [
        "submit_case1_1 = run_model(X_data,X_data_test, Y_data, 1,True,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkENIFIZBkKr"
      },
      "source": [
        "# Case2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTlddofkBkgn"
      },
      "source": [
        "X_data = X_data_Case12.copy()\n",
        "X_data_test = X_data_test_Case12.copy()\n",
        "Y_data = Y_data_Case12.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdeKwBNYBlSf"
      },
      "source": [
        "submit_case2_1 = run_model(X_data,X_data_test, Y_data, 4,False,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7khj-lkBmj0"
      },
      "source": [
        "# Case 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fM0JHPIBnuZ"
      },
      "source": [
        "X_data = X_data_Case3.copy()\n",
        "X_data_test = X_data_test_Case3.copy()\n",
        "Y_data = Y_data_Case3.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTyW2Sm6BoeX"
      },
      "source": [
        "submit_case3_1 = run_model(X_data,X_data_test, Y_data, 7,False,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyAAINNJBpRn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBp83HEnBqL0"
      },
      "source": [
        "# Case 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDSytt2KBq5V"
      },
      "source": [
        "X_data = X_data_Case4.copy()\n",
        "X_data_test = X_data_test_Case4.copy()\n",
        "Y_data = Y_data_Case4.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vbdjeebBr45"
      },
      "source": [
        "submit_case4_1 = run_model(X_data,X_data_test, Y_data, 10,True,False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}